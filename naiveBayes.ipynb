{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import orjson\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"train_topics_keywords.tsv\"\n",
    "keywords = {}\n",
    "tp = {}\n",
    "with open(path, encoding=\"utf-8\") as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for line in tsv_file:\n",
    "        keywords[line[1]] = line[2].split(\",\")\n",
    "        tp[line[0]] = line[1]\n",
    "# print(keywords)\n",
    "\n",
    "path = \"stem_nostop\\\\topics_stem_nostop.json\"\n",
    "with open(path, 'wb') as f:\n",
    "    f.write(orjson.dumps(tp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tsv to doc_id --> topic dict\n",
    "path = \"train_topics_reldocs.tsv\"\n",
    "keywords = {}\n",
    "total = set()\n",
    "doc_to_topics = {}\n",
    "with open(path) as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for line in tsv_file:\n",
    "        # print(line)\n",
    "        # print(line[1])\n",
    "        keywords[line[1]] = line[2].split(\",\")\n",
    "        for id in line[2].split(\",\"):\n",
    "            if id in doc_to_topics:\n",
    "                doc_to_topics[id].append(line[0])\n",
    "            else:\n",
    "                doc_to_topics[id] = [line[0]]\n",
    "            total.add(id)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"stem_nostop/cleanDocsDict_stem_nostop_true.json\"\n",
    "# path = \"stem_nostop\\cleanDocsDict_stem_nostop.json\"\n",
    "with open(path, 'rb') as f:\n",
    "    cd_keys = orjson.loads(f.read()).keys()\n",
    "\n",
    "\n",
    "# Convert tsv to topic --> doc_id[] dict\n",
    "path = \"train_topics_reldocs.tsv\"\n",
    "keywords = {}\n",
    "total = set()\n",
    "topics_to_doc = {}\n",
    "with open(path) as file:\n",
    "    tsv_file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for line in tsv_file:\n",
    "        all = line[2].split(\",\")\n",
    "        rel = []\n",
    "        for doc in all:\n",
    "            if doc in cd_keys:\n",
    "                rel.append(doc)\n",
    "        topics_to_doc[line[1]] = rel\n",
    "        \n",
    "\n",
    "path = \"stem_nostop/topics_to_doc_stem_nostop.json\"\n",
    "with open(path, 'wb') as f:\n",
    "    f.write(orjson.dumps(topics_to_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"stem_nostop/cleanDocsDict_stem_nostop_true.json\"\n",
    "with open(path, 'rb') as f:\n",
    "    cd = orjson.loads(f.read())\n",
    "\n",
    "# set of words in the\n",
    "vocab = set()\n",
    "\n",
    "# total doc count\n",
    "doc_count = 0\n",
    "\n",
    "# dict of topic --> doc count\n",
    "docs_per_topic = {}\n",
    "\n",
    "# dict of topic --> term count\n",
    "terms_per_topic = {}\n",
    "\n",
    "# dict of term --> dict of topic --> count\n",
    "term_topic_counts = {}\n",
    "\n",
    "\n",
    "\n",
    "for doc_id, terms in cd.items():\n",
    "    doc_count += 1\n",
    "    terms = terms.split(\" \")\n",
    "    topics = doc_to_topics[doc_id]\n",
    "    for topic in topics:\n",
    "        docs_per_topic[topic] = 1 + docs_per_topic.get(topic, 0)\n",
    "        \n",
    "        for term in terms:\n",
    "            vocab.add(term)\n",
    "            terms_per_topic[topic] = 1 + terms_per_topic.get(topic, 0)\n",
    "            if term not in term_topic_counts:\n",
    "                term_topic_counts[term] = {}\n",
    "            term_topic_counts[term][topic] = 1 + term_topic_counts[term].get(topic, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior = {}\n",
    "for topic, count in docs_per_topic.items():\n",
    "    prior[topic] = math.log((count/doc_count))\n",
    "\n",
    "condprob = {}\n",
    "# count of terms in rel doc that arn't the term\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "for term in vocab:\n",
    "    # if not initialized, do that\n",
    "    if term not in condprob:\n",
    "        condprob[term] = {}\n",
    "    # for each topic calculate the cond prob for the given term\n",
    "    for topic in docs_per_topic.keys():\n",
    "        # nom = condprob[term].get(topic, 0)+1\n",
    "        nom = term_topic_counts[term].get(topic, 0) + 1\n",
    "        denom = terms_per_topic[topic] + vocab_len\n",
    "        condprob[term][topic] = math.log(nom/denom)\n",
    "\n",
    "path = \"stem_nostop/NB_stem_nostop.json\"\n",
    "with open(path, 'wb') as f:\n",
    "    f.write(orjson.dumps({\"prior\": prior, \"condprob\": condprob, \"doc_to_topics\": doc_to_topics, \"topics\": tp}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
